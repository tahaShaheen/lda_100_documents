{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb86b56",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde91e3",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Develop a program satisfying the following requirements\n",
    "\n",
    "- Analyze the topics of documents used in Assignment 1\n",
    "- Use LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b516ef0",
   "metadata": {},
   "source": [
    "## Report\n",
    "- Content\n",
    "- Summary of your dataset\n",
    "- Analysis results\n",
    "    - List top 10 keywords per each topic\n",
    "    - Choose 10 documents and show their topic distribution.\n",
    "    - Compare the results with different topic numbers.\n",
    "- Thoughts/Impressions\n",
    "- Deadline: 10:30, May 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4e383",
   "metadata": {},
   "source": [
    "### Summary of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16cc973",
   "metadata": {},
   "source": [
    "My dataset consists of 100 wikipedia articles. All are imported using the wikipedia api for python.\n",
    "<br>\n",
    "The list is below:\n",
    " - Animals\n",
    "     - Domestic Dogs\n",
    "     - Cats\n",
    "     - Ducklings\n",
    "     - Goose\n",
    "     - Zebra\n",
    "     - Elephant\n",
    "     - Stork\n",
    "     - Lion\n",
    "     - Tiger\n",
    "     - Cattle\n",
    " - US Presidents\n",
    "     - George Washington\n",
    "     - Thomas Jefferson\n",
    "     - Abraham Lincoln\n",
    "     - Ulysses S. Grant\n",
    "     - Theodore Roosevelt\n",
    "     - Franklin D. Roosevelt\n",
    "     - John Fitzgerald Kennedy\n",
    "     - George Walker Bush\n",
    "     - Barack Obama\n",
    "     - Donald Trump\n",
    " - Mountains\n",
    "     - Mount Everest \n",
    "     - K2\n",
    "     - Kangchenjunga\n",
    "     - Lhotse\n",
    "     - Makalu\n",
    "     - Cho Oyu\n",
    "     - Dhaulagiri I\n",
    "     - Manaslu\n",
    "     - Nanga Parbat\n",
    "     - Annapurna I\n",
    " - Websites\n",
    "     - YouTube \n",
    "     - Google \n",
    "     - Facebook \n",
    "     - Tinder \n",
    "     - Wikipedia \n",
    "     - Vimeo \n",
    "     - GitHub \n",
    "     - Apple Inc. \n",
    "     - PayPal \n",
    "     - Microsoft\n",
    " - Sports\n",
    "     - Football \n",
    "     - Basketball \n",
    "     - Cricket \n",
    "     - Baseball \n",
    "     - Swimming \n",
    "     - Badminton \n",
    "     - Tennis \n",
    "     - Volleyball \n",
    "     - Golf \n",
    "     - Surfing\n",
    " - Authors\n",
    "     - Isaac Asimov \n",
    "     - Arthur C. Clarke \n",
    "     - H. G. Wells \n",
    "     - Robert A. Heinlein \n",
    "     - Frank Herbert \n",
    "     - J. R. R. Tolkien \n",
    "     - J. K. Rowling \n",
    "     - George R. R. Martin \n",
    "     - Ursula K. Le Guin \n",
    "     - N. K. Jemisin\n",
    " - Bollywood Actors\n",
    "     - Shahrukh Khan \n",
    "     - Amitabh Bachchan \n",
    "     - Irrfan Khan \n",
    "     - Aamir Khan \n",
    "     - Ranbir Kapoor \n",
    "     - Priyanka Chopra \n",
    "     - Deepika Padukone \n",
    "     - Alia Bhatt \n",
    "     - Anushka Sharma \n",
    "     - Kareena Kapoor\n",
    " - Hollywood Actors\n",
    "     - Tom Hanks \n",
    "     - Leonardo DiCaprio \n",
    "     - Will Smith \n",
    "     - Tom Cruise \n",
    "     - Dwayne Johnson \n",
    "     - Scarlett Johansson \n",
    "     - Natalie Portman \n",
    "     - Charlize Theron \n",
    "     - Jennifer Lawrence \n",
    "     - Zendaya\n",
    " - Video Games\n",
    "     - Minecraft \n",
    "     - Fortnite \n",
    "     - Grand Theft Auto V \n",
    "     - World of Warcraft \n",
    "     - Among Us \n",
    "     - Overwatch (video game) \n",
    "     - PlayerUnknown\\'s Battlegrounds \n",
    "     - Pokémon GO \n",
    "     - Red Dead Redemption 2 \n",
    "     - The Last of Us\n",
    " - Aiplane Crashes\n",
    "     - Malaysia Airlines Flight 370 \n",
    "     - Malaysia Airlines Flight 17 \n",
    "     - Japan Airlines Flight 123 \n",
    "     - Lion Air Flight 610 \n",
    "     - Germanwings Flight 9525 \n",
    "     - American Airlines Flight 191 \n",
    "     - Tenerife airport disaster \n",
    "     - Ethiopian Airlines Flight 302 \n",
    "     - Air France Flight 447 \n",
    "     - Pakistan International Airlines Flight 8303"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae741a",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525bc725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taha/anaconda3/envs/analysis/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import wikipedia\n",
    "import re\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text, strip_non_alphanum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236298ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_articles=[]\n",
    "titles=[]\n",
    "actual_clusters = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20230c36",
   "metadata": {},
   "source": [
    "Uncomment to download the articles. Right now articles are saved in a pickle file so no need to download again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c5c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_animals=[\n",
    "#     'Domestic dogs',\n",
    "#     'Cat',\n",
    "#     'Duckling',\n",
    "#     'Goose',\n",
    "#     'Zebra',\n",
    "#     'Elephant',\n",
    "#     'Stork',\n",
    "#     'Lion',\n",
    "#     'Tiger',\n",
    "#     'Cattle'\n",
    "# ]\n",
    "\n",
    "# articles_us_presidents=[\n",
    "#     'George Washington',\n",
    "#     'Thomas Jefferson',\n",
    "#     'Abraham Lincoln',\n",
    "#     'Ulysses S. Grant',\n",
    "#     'Theodore Roosevelt',\n",
    "#     'Franklin D. Roosevelt',\n",
    "#     'John Fitzgerald Kennedy',\n",
    "#     'George Walker Bush',\n",
    "#     'Barack Obama',\n",
    "#     'Donald Trump'\n",
    "# ]\n",
    "\n",
    "# articles_mountains=[\n",
    "#     'Mount Everest',\n",
    "#     'K2',\n",
    "#     'Kangchenjunga',\n",
    "#     'Lhotse',\n",
    "#     'Makalu',\n",
    "#     'Cho Oyu',\n",
    "#     'Dhaulagiri I',\n",
    "#     'Manaslu',\n",
    "#     'Nanga Parbat',\n",
    "#     'Annapurna I'\n",
    "# ]\n",
    "\n",
    "# articles_websites=[\n",
    "#     'YouTube',\n",
    "#     'Google',\n",
    "#     'Facebook',\n",
    "#     'Tinder',\n",
    "#     'Wikipedia',\n",
    "#     'Vimeo',\n",
    "#     'GitHub',\n",
    "#     'Apple Inc.',\n",
    "#     'PayPal',\n",
    "#     'Microsoft'\n",
    "# ]\n",
    "\n",
    "# articles_sports = [\n",
    "#     'Football',\n",
    "#     'Basketball',\n",
    "#     'Cricket',\n",
    "#     'Baseball',\n",
    "#     'Swimming',\n",
    "#     'Badminton',\n",
    "#     'Tennis',\n",
    "#     'Volleyball',\n",
    "#     'Golf',\n",
    "#     'Surfing'\n",
    "# ]\n",
    "\n",
    "# articles_authors = [\n",
    "#     'Isaac Asimov',\n",
    "#     'Arthur C. Clarke',\n",
    "#     'H. G. Wells',\n",
    "#     'Robert A. Heinlein',\n",
    "#     'Frank Herbert',\n",
    "#     'J. R. R. Tolkien',\n",
    "#     'J. K. Rowling',\n",
    "#     'George R. R. Martin',\n",
    "#     'Ursula K. Le Guin',\n",
    "#     'N. K. Jemisin'\n",
    "# ]\n",
    "\n",
    "# articles_actors_bollywood = [\n",
    "#     'Shahrukh Khan',\n",
    "#     'Amitabh Bachchan',\n",
    "#     'Irrfan Khan',\n",
    "#     'Aamir Khan',\n",
    "#     'Ranbir Kapoor',\n",
    "#     'Priyanka Chopra',\n",
    "#     'Deepika Padukone',\n",
    "#     'Alia Bhatt',\n",
    "#     'Anushka Sharma',\n",
    "#     'Kareena Kapoor'\n",
    "# ]\n",
    "\n",
    "# articles_actors_hollywood = [\n",
    "#     'Tom Hanks',\n",
    "#     'Leonardo DiCaprio',\n",
    "#     'Will Smith',\n",
    "#     'Tom Cruise',\n",
    "#     'Dwayne Johnson',\n",
    "#     'Scarlett Johansson',\n",
    "#     'Natalie Portman',\n",
    "#     'Charlize Theron',\n",
    "#     'Jennifer Lawrence',\n",
    "#     'Zendaya'\n",
    "# ]\n",
    "\n",
    "# articles_video_games = [\n",
    "#     'Minecraft',\n",
    "#     'Fortnite',\n",
    "#     'Grand Theft Auto V',\n",
    "#     'World of Warcraft',\n",
    "#     'Among Us',\n",
    "#     'Overwatch (video game)',\n",
    "#     'PlayerUnknown\\'s Battlegrounds',\n",
    "#     'Pokémon GO',\n",
    "#     'Red Dead Redemption 2',\n",
    "#     'The Last of Us'\n",
    "# ]\n",
    "\n",
    "# articles_airplane_crashes = [\n",
    "#     'Malaysia Airlines Flight 370',\n",
    "#     'Malaysia Airlines Flight 17',\n",
    "#     'Japan Airlines Flight 123',\n",
    "#     'Lion Air Flight 610',\n",
    "#     'Germanwings Flight 9525',\n",
    "#     'American Airlines Flight 191',\n",
    "#     'Tenerife airport disaster',\n",
    "#     'Ethiopian Airlines Flight 302',\n",
    "#     'Air France Flight 447',\n",
    "#     'Pakistan International Airlines Flight 8303'\n",
    "# ]\n",
    "\n",
    "\n",
    "# all_articles = zip(articles_animals, \n",
    "#                    articles_us_presidents, \n",
    "#                    articles_mountains, \n",
    "#                    articles_websites, \n",
    "#                    articles_sports,\n",
    "#                    articles_authors,\n",
    "#                    articles_actors_bollywood,\n",
    "#                    articles_actors_hollywood,\n",
    "#                    articles_video_games,\n",
    "#                    articles_airplane_crashes\n",
    "#                   )\n",
    "\n",
    "# for article in all_articles:\n",
    "#     print(\"loading content: \",article)\n",
    "#     wikipedia_articles.append((wikipedia.page(article[0], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[1], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[2], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[3], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[4], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[5], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[6], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[7], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[8], auto_suggest=False).content))\n",
    "#     wikipedia_articles.append((wikipedia.page(article[9], auto_suggest=False).content))\n",
    "#     titles.append(article[0])\n",
    "#     titles.append(article[1])\n",
    "#     titles.append(article[2])\n",
    "#     titles.append(article[3])\n",
    "#     titles.append(article[4])\n",
    "#     titles.append(article[5])\n",
    "#     titles.append(article[6])\n",
    "#     titles.append(article[7])\n",
    "#     titles.append(article[8])\n",
    "#     titles.append(article[9])\n",
    "#     actual_clusters.append('Animal')\n",
    "#     actual_clusters.append('US President')\n",
    "#     actual_clusters.append('Mountain')\n",
    "#     actual_clusters.append('Website')\n",
    "#     actual_clusters.append('Sport')\n",
    "#     actual_clusters.append('Author')\n",
    "#     actual_clusters.append('Bollywood Actor')\n",
    "#     actual_clusters.append('Hollywood Actor')\n",
    "#     actual_clusters.append('Video Game')\n",
    "#     actual_clusters.append('Airplane Crash')\n",
    "    \n",
    "# print('done')\n",
    "\n",
    "\n",
    "# downloaded_articles_dictionary = {\n",
    "#     \"wikipedia_articles\": wikipedia_articles,\n",
    "#     \"titles\": titles,\n",
    "#     \"actual_clusters\" : actual_clusters\n",
    "# }\n",
    "\n",
    "# saved_artciles_file = open(\"saved_artciles_file.pkl\", \"wb\")\n",
    "# pickle.dump(downloaded_articles_dictionary, saved_artciles_file)\n",
    "# saved_artciles_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db20a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_artciles_file = open(\"saved_artciles_file.pkl\", \"rb\")\n",
    "downloaded_articles_dictionary = pickle.load(saved_artciles_file)\n",
    "saved_artciles_file.close()\n",
    "\n",
    "wikipedia_articles = downloaded_articles_dictionary[\"wikipedia_articles\"]\n",
    "titles = downloaded_articles_dictionary[\"titles\"]\n",
    "actual_clusters = downloaded_articles_dictionary[\"actual_clusters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fef6f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 documents imported\n"
     ]
    }
   ],
   "source": [
    "print(len(wikipedia_articles), \"documents imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4cbf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling entries just to make sure that order has nothing to do with anything\n",
    "z = list(zip(wikipedia_articles, titles, actual_clusters))\n",
    "random.shuffle(z)\n",
    "wikipedia_articles, titles, actual_clusters = zip(*z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51db235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(wikipedia_articles[0])\n",
    "# preprocessed_wiki_article = preprocess_documents(wikipedia_articles)\n",
    "\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric, \n",
    "           strip_multiple_whitespaces, strip_non_alphanum, remove_stopwords, stem_text, lambda x: strip_short(x, minsize=4)]\n",
    "\n",
    "preprocessed_wiki_articles = [preprocess_string(article, filters) for article in wikipedia_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c402b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing the first 50 entires in article with title: Tom Hanks\n",
      "==============================================================================================\n",
      "['thoma', 'jeffrei', 'hank', 'born', 'juli', 'american', 'actor', 'filmmak', 'known', 'comed', 'dramat', 'role', 'popular', 'recogniz', 'film', 'star', 'worldwid', 'regard', 'american', 'cultur', 'icon', 'hank', 'film', 'gross', 'billion', 'north', 'america', 'billion', 'worldwid', 'make', 'fourth', 'highest', 'gross', 'actor', 'north', 'america', 'hank', 'breakthrough', 'lead', 'role', 'comedi', 'splash', 'consecut', 'academi', 'award', 'best', 'actor', 'star', 'lawyer', 'suffer']\n"
     ]
    }
   ],
   "source": [
    "print(\"printing the first 50 entires in article with title:\", titles[0])\n",
    "print(\"==============================================================================================\")\n",
    "print(preprocessed_wiki_articles[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6847aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dictionary\n",
    "dictionary = corpora.Dictionary(preprocessed_wiki_articles)\n",
    "\n",
    "# construct the corpus\n",
    "corpus_ = [dictionary.doc2bow(doc) for doc in preprocessed_wiki_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "347cdf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volunt (1157, 1)\n",
      "vote (1158, 5)\n",
      "voyag (1159, 1)\n",
      "wachowski (1160, 1)\n",
      "wait (1161, 2)\n",
      "waldman (1162, 1)\n",
      "walk (1163, 1)\n",
      "wall (1164, 1)\n",
      "wallner (1165, 1)\n",
      "walt (1166, 2)\n",
      "want (1167, 2)\n",
      "wash (1168, 1)\n",
      "watch (1169, 2)\n",
      "watson (1170, 1)\n",
      "week (1171, 1)\n",
      "weekend (1172, 1)\n",
      "went (1173, 5)\n",
      "wernher (1174, 1)\n",
      "western (1175, 1)\n",
      "white (1176, 2)\n",
      "widow (1177, 1)\n",
      "wife (1178, 4)\n",
      "wild (1179, 1)\n",
      "wildfir (1180, 1)\n",
      "will (1181, 1)\n",
      "william (1182, 1)\n",
      "wilson (1183, 11)\n",
      "wisecrack (1184, 1)\n",
      "woman (1185, 1)\n",
      "women (1186, 1)\n",
      "wong (1187, 1)\n",
      "woodi (1188, 5)\n",
      "work (1189, 12)\n",
      "worker (1190, 1)\n",
      "world (1191, 8)\n",
      "worldwid (1192, 6)\n",
      "wouldn (1193, 1)\n",
      "wrap (1194, 1)\n",
      "write (1195, 4)\n",
      "writer (1196, 3)\n",
      "written (1197, 1)\n",
      "wrote (1198, 5)\n",
      "wwii (1199, 1)\n",
      "year (1200, 19)\n",
      "yell (1201, 1)\n",
      "york (1202, 3)\n",
      "yorker (1203, 2)\n",
      "young (1204, 3)\n",
      "youngest (1205, 3)\n",
      "zemecki (1206, 3)\n"
     ]
    }
   ],
   "source": [
    "# corpus_ contains words of each document with a list (ID, appear frequency)\n",
    "# note that there is not the appearing order in the documents, but the order of the dictionary\n",
    "\n",
    "# printing the last 50 dictionary entires\n",
    "for i, _ in corpus_[0][-50:]:\n",
    "    print(dictionary[i], corpus_[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d33836",
   "metadata": {},
   "source": [
    "## Analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a13fe",
   "metadata": {},
   "source": [
    "### Compare the results with different topic number (number of topics:  20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feda5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus_,\n",
    "                                           num_topics=20,\n",
    "                                           id2word=dictionary,\n",
    "                                           alpha=0.01,                 # optional LDA hyperparameter alpha\n",
    "                                           eta=0.01,                   # optional LDA hyperparameter beta\n",
    "                                           #minimum_probability=0.0    # optional the lower bound of the topic/word generative probability\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce6274",
   "metadata": {},
   "source": [
    "### List top 10 keywords per each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ce705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0\n",
      "=====================================================================================\n",
      "['game', 'plai', 'roosevelt', 'asimov', 'state', 'player', 'ball', 'jefferson', 'year', 'american']\n",
      "=====================================================================================\n",
      "Topic number 1\n",
      "=====================================================================================\n",
      "['game', 'player', 'year', 'includ', 'time', 'state', 'plai', 'work', 'eleph', 'world']\n",
      "=====================================================================================\n",
      "Topic number 2\n",
      "=====================================================================================\n",
      "['state', 'film', 'grant', 'game', 'year', 'includ', 'time', 'roosevelt', 'trump', 'flight']\n",
      "=====================================================================================\n",
      "Topic number 3\n",
      "=====================================================================================\n",
      "['roosevelt', 'film', 'year', 'game', 'award', 'player', 'world', 'state', 'plai', 'time']\n",
      "=====================================================================================\n",
      "Topic number 4\n",
      "=====================================================================================\n",
      "['film', 'game', 'year', 'state', 'grant', 'time', 'american', 'nation', 'presid', 'work']\n",
      "=====================================================================================\n",
      "Topic number 5\n",
      "=====================================================================================\n",
      "['time', 'film', 'year', 'includ', 'award', 'state', 'player', 'aircraft', 'kennedi', 'report']\n",
      "=====================================================================================\n",
      "Topic number 6\n",
      "=====================================================================================\n",
      "['film', 'game', 'year', 'time', 'state', 'award', 'includ', 'world', 'critic', 'best']\n",
      "=====================================================================================\n",
      "Topic number 7\n",
      "=====================================================================================\n",
      "['game', 'player', 'time', 'appl', 'year', 'world', 'plai', 'film', 'includ', 'team']\n",
      "=====================================================================================\n",
      "Topic number 8\n",
      "=====================================================================================\n",
      "['game', 'year', 'time', 'player', 'plai', 'includ', 'team', 'state', 'flight', 'world']\n",
      "=====================================================================================\n",
      "Topic number 9\n",
      "=====================================================================================\n",
      "['year', 'state', 'time', 'film', 'game', 'presid', 'includ', 'player', 'world', 'work']\n",
      "=====================================================================================\n",
      "Topic number 10\n",
      "=====================================================================================\n",
      "['film', 'player', 'game', 'year', 'award', 'team', 'time', 'state', 'plai', 'includ']\n",
      "=====================================================================================\n",
      "Topic number 11\n",
      "=====================================================================================\n",
      "['game', 'includ', 'state', 'grant', 'video', 'plai', 'year', 'nation', 'lincoln', 'player']\n",
      "=====================================================================================\n",
      "Topic number 12\n",
      "=====================================================================================\n",
      "['plai', 'game', 'time', 'ball', 'state', 'player', 'flight', 'trump', 'includ', 'footbal']\n",
      "=====================================================================================\n",
      "Topic number 13\n",
      "=====================================================================================\n",
      "['game', 'player', 'plai', 'time', 'releas', 'year', 'film', 'team', 'state', 'ball']\n",
      "=====================================================================================\n",
      "Topic number 14\n",
      "=====================================================================================\n",
      "['wikipedia', 'game', 'year', 'player', 'includ', 'appl', 'time', 'lion', 'film', 'releas']\n",
      "=====================================================================================\n",
      "Topic number 15\n",
      "=====================================================================================\n",
      "['game', 'roosevelt', 'time', 'includ', 'film', 'state', 'year', 'plai', 'trump', 'nation']\n",
      "=====================================================================================\n",
      "Topic number 16\n",
      "=====================================================================================\n",
      "['film', 'game', 'kapoor', 'award', 'year', 'time', 'state', 'star', 'plai', 'khan']\n",
      "=====================================================================================\n",
      "Topic number 17\n",
      "=====================================================================================\n",
      "['game', 'time', 'footbal', 'year', 'plai', 'film', 'state', 'million', 'includ', 'player']\n",
      "=====================================================================================\n",
      "Topic number 18\n",
      "=====================================================================================\n",
      "['film', 'game', 'state', 'million', 'year', 'includ', 'compani', 'time', 'youtub', 'award']\n",
      "=====================================================================================\n",
      "Topic number 19\n",
      "=====================================================================================\n",
      "['film', 'year', 'time', 'award', 'game', 'state', 'best', 'million', 'includ', 'work']\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "lda_topics = ldamodel.print_topics(num_words = 10)\n",
    "\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "topic_dictionary = {}\n",
    "\n",
    "for topic in lda_topics:\n",
    "    print(\"Topic number\", topic[0])\n",
    "    print(\"=====================================================================================\")\n",
    "#     print(topic)\n",
    "    print(preprocess_string(topic[1], filters))\n",
    "    topic_dictionary[(topic[0])] = preprocess_string(topic[1], filters)\n",
    "    print(\"=====================================================================================\")  \n",
    "\n",
    "# print(topic_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbccf0",
   "metadata": {},
   "source": [
    "### Choose 10 documents and show their topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67ebaf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document ID 0 | Hollywood Actor | Tom Hanks | [(16, 0.044564582), (19, 0.94820505)]\n",
      "document ID 1 | Airplane Crash | Tenerife airport disaster | [(12, 0.9999099)]\n",
      "document ID 2 | Author | Frank Herbert | [(6, 0.027623603), (9, 0.035298716), (14, 0.15019822), (16, 0.78677917)]\n",
      "document ID 3 | Mountain | Annapurna I | [(6, 0.9997564)]\n",
      "document ID 4 | Website | PayPal | [(2, 0.9483779), (14, 0.051567134)]\n",
      "document ID 5 | US President | Thomas Jefferson | [(0, 0.99730533)]\n",
      "document ID 6 | US President | George Washington | [(0, 0.8151672), (2, 0.07954566), (6, 0.09934408)]\n",
      "document ID 7 | Sport | Badminton | [(13, 0.9999411)]\n",
      "document ID 8 | Bollywood Actor | Amitabh Bachchan | [(2, 0.029398482), (10, 0.44287002), (16, 0.5273335)]\n",
      "document ID 9 | Author | Ursula K. Le Guin | [(6, 0.92162514), (16, 0.06944372)]\n"
     ]
    }
   ],
   "source": [
    "# for each document, show the probabilities of topics which beyond the minimum_probability [(topic ID, probability)]\n",
    "\n",
    "for n,item in enumerate(corpus_[:10]):\n",
    "    print(\"document ID\", str(n), \"|\" ,end=\" \")\n",
    "    print(actual_clusters[n],end=\" | \")\n",
    "    print(titles[n],end=\" \")\n",
    "    print(\"|\", ldamodel.get_document_topics(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3887ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random artcile number = 9\n",
      "(6, 0.914257) | ['film', 'game', 'year', 'time', 'state', 'award', 'includ', 'world', 'critic', 'best']\n",
      "(16, 0.07634851) | ['film', 'game', 'kapoor', 'award', 'year', 'time', 'state', 'star', 'plai', 'khan']\n",
      "Author | Ursula K. Le Guin\n",
      "Ursula Kroeber Le Guin (; October 21, 1929 – January 22, 2018) was an American author best known for her works of speculative fiction, including science fiction works set in her Hainish universe, and the Earthsea fantasy series. She was first published in 1959, and her literary career spanned nearly sixty years, producing more than twenty novels and over a hundred short stories, in addition to poetry, literary criticism, translations, and children's books. Frequently described as an author of sc\n"
     ]
    }
   ],
   "source": [
    "random_artcile = random.choice(range(len(wikipedia_articles)))\n",
    "\n",
    "# nth document's topic distribution\n",
    "topic_distribution = ldamodel.get_document_topics(corpus_[random_artcile])\n",
    "\n",
    "print(\"random artcile number =\", random_artcile)\n",
    "\n",
    "for topic in topic_distribution:\n",
    "    print(topic, \"|\", topic_dictionary[topic[0]])\n",
    "\n",
    "# nth document's category\n",
    "print(actual_clusters[random_artcile], \"|\", titles[random_artcile])\n",
    "\n",
    "# show the original document\n",
    "print(wikipedia_articles[random_artcile][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33aefa1",
   "metadata": {},
   "source": [
    "### Compare the results with different topic number (number of topics now:  10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7e67c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus_,\n",
    "                                           num_topics=10,\n",
    "                                           id2word=dictionary,\n",
    "                                           alpha=0.01,                 # optional LDA hyperparameter alpha\n",
    "                                           eta=0.01,                   # optional LDA hyperparameter beta\n",
    "                                           #minimum_probability=0.0    # optional the lower bound of the topic/word generative probability\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5258a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0\n",
      "=====================================================================================\n",
      "['game', 'state', 'year', 'player', 'includ', 'releas', 'plai', 'time', 'appl', 'award']\n",
      "=====================================================================================\n",
      "Topic number 1\n",
      "=====================================================================================\n",
      "['game', 'time', 'footbal', 'plai', 'player', 'state', 'year', 'ball', 'team', 'wikipedia']\n",
      "=====================================================================================\n",
      "Topic number 2\n",
      "=====================================================================================\n",
      "['film', 'year', 'game', 'plai', 'time', 'world', 'includ', 'flight', 'award', 'work']\n",
      "=====================================================================================\n",
      "Topic number 3\n",
      "=====================================================================================\n",
      "['film', 'year', 'grant', 'state', 'time', 'award', 'game', 'plai', 'million', 'trump']\n",
      "=====================================================================================\n",
      "Topic number 4\n",
      "=====================================================================================\n",
      "['film', 'plai', 'time', 'includ', 'game', 'state', 'year', 'award', 'critic', 'role']\n",
      "=====================================================================================\n",
      "Topic number 5\n",
      "=====================================================================================\n",
      "['game', 'player', 'year', 'film', 'includ', 'state', 'time', 'plai', 'nation', 'washington']\n",
      "=====================================================================================\n",
      "Topic number 6\n",
      "=====================================================================================\n",
      "['game', 'includ', 'time', 'year', 'state', 'film', 'lincoln', 'plai', 'player', 'releas']\n",
      "=====================================================================================\n",
      "Topic number 7\n",
      "=====================================================================================\n",
      "['game', 'player', 'year', 'film', 'time', 'includ', 'video', 'plai', 'state', 'best']\n",
      "=====================================================================================\n",
      "Topic number 8\n",
      "=====================================================================================\n",
      "['roosevelt', 'game', 'state', 'time', 'film', 'presid', 'year', 'trump', 'includ', 'world']\n",
      "=====================================================================================\n",
      "Topic number 9\n",
      "=====================================================================================\n",
      "['game', 'player', 'time', 'plai', 'ball', 'state', 'includ', 'team', 'award', 'year']\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "lda_topics = ldamodel.print_topics(num_words = 10)\n",
    "\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "topic_dictionary = {}\n",
    "\n",
    "for topic in lda_topics:\n",
    "    print(\"Topic number\", topic[0])\n",
    "    print(\"=====================================================================================\")\n",
    "#     print(topic)\n",
    "    print(preprocess_string(topic[1], filters))\n",
    "    topic_dictionary[(topic[0])] = preprocess_string(topic[1], filters)\n",
    "    print(\"=====================================================================================\")  \n",
    "\n",
    "# print(topic_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc254386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document ID 0 | Hollywood Actor | Tom Hanks | [(3, 0.99637896)]\n",
      "document ID 1 | Airplane Crash | Tenerife airport disaster | [(0, 0.054794963), (2, 0.6512695), (3, 0.28217867), (5, 0.011586371)]\n",
      "document ID 2 | Author | Frank Herbert | [(0, 0.5336694), (2, 0.4662813)]\n",
      "document ID 3 | Mountain | Annapurna I | [(6, 0.9998847)]\n",
      "document ID 4 | Website | PayPal | [(0, 0.28271586), (1, 0.02069597), (2, 0.6892153)]\n",
      "document ID 5 | US President | Thomas Jefferson | [(0, 0.086690314), (7, 0.031960703), (8, 0.8757747)]\n",
      "document ID 6 | US President | George Washington | [(3, 0.0113578765), (5, 0.13449107), (8, 0.85414255)]\n",
      "document ID 7 | Sport | Badminton | [(0, 0.070898116), (5, 0.05615337), (9, 0.87286484)]\n",
      "document ID 8 | Bollywood Actor | Amitabh Bachchan | [(0, 0.59402835), (3, 0.38009086), (4, 0.025859125)]\n",
      "document ID 9 | Author | Ursula K. Le Guin | [(2, 0.4621686), (8, 0.41998917), (9, 0.117825255)]\n"
     ]
    }
   ],
   "source": [
    "# for each document, show the probabilities of topics which beyond the minimum_probability [(topic ID, probability)]\n",
    "\n",
    "for n,item in enumerate(corpus_[:10]):\n",
    "    print(\"document ID\", str(n), \"|\" ,end=\" \")\n",
    "    print(actual_clusters[n],end=\" | \")\n",
    "    print(titles[n],end=\" \")\n",
    "    print(\"|\", ldamodel.get_document_topics(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38d66e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random artcile number = 9\n",
      "(2, 0.45115054) | ['film', 'year', 'game', 'plai', 'time', 'world', 'includ', 'flight', 'award', 'work']\n",
      "(8, 0.43897215) | ['roosevelt', 'game', 'state', 'time', 'film', 'presid', 'year', 'trump', 'includ', 'world']\n",
      "(9, 0.1098603) | ['game', 'player', 'time', 'plai', 'ball', 'state', 'includ', 'team', 'award', 'year']\n",
      "Author | Ursula K. Le Guin\n",
      "Ursula Kroeber Le Guin (; October 21, 1929 – January 22, 2018) was an American author best known for her works of speculative fiction, including science fiction works set in her Hainish universe, and the Earthsea fantasy series. She was first published in 1959, and her literary career spanned nearly sixty years, producing more than twenty novels and over a hundred short stories, in addition to poetry, literary criticism, translations, and children's books. Frequently described as an author of sc\n"
     ]
    }
   ],
   "source": [
    "# nth document's topic distribution\n",
    "topic_distribution = ldamodel.get_document_topics(corpus_[random_artcile])\n",
    "\n",
    "print(\"random artcile number =\", random_artcile)\n",
    "\n",
    "for topic in topic_distribution:\n",
    "    print(topic, \"|\", topic_dictionary[topic[0]])\n",
    "\n",
    "# nth document's category\n",
    "print(actual_clusters[random_artcile], \"|\", titles[random_artcile])\n",
    "\n",
    "# show the original document\n",
    "print(wikipedia_articles[random_artcile][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce0a66",
   "metadata": {},
   "source": [
    "### Compare the results with different topic number (number of topics now:  10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44def722",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus_,\n",
    "                                           num_topics=5,\n",
    "                                           id2word=dictionary,\n",
    "                                           alpha=0.01,                 # optional LDA hyperparameter alpha\n",
    "                                           eta=0.01,                   # optional LDA hyperparameter beta\n",
    "                                           #minimum_probability=0.0    # optional the lower bound of the topic/word generative probability\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9a9bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number 0\n",
      "=====================================================================================\n",
      "['game', 'time', 'state', 'year', 'facebook', 'plai', 'includ', 'grant', 'film', 'million']\n",
      "=====================================================================================\n",
      "Topic number 1\n",
      "=====================================================================================\n",
      "['game', 'player', 'video', 'state', 'world', 'includ', 'time', 'year', 'film', 'roosevelt']\n",
      "=====================================================================================\n",
      "Topic number 2\n",
      "=====================================================================================\n",
      "['game', 'film', 'award', 'time', 'year', 'state', 'player', 'includ', 'world', 'releas']\n",
      "=====================================================================================\n",
      "Topic number 3\n",
      "=====================================================================================\n",
      "['game', 'film', 'player', 'year', 'includ', 'plai', 'time', 'state', 'ball', 'releas']\n",
      "=====================================================================================\n",
      "Topic number 4\n",
      "=====================================================================================\n",
      "['film', 'year', 'game', 'plai', 'state', 'player', 'time', 'includ', 'award', 'nation']\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "lda_topics = ldamodel.print_topics(num_words = 10)\n",
    "\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "topic_dictionary = {}\n",
    "\n",
    "for topic in lda_topics:\n",
    "    print(\"Topic number\", topic[0])\n",
    "    print(\"=====================================================================================\")\n",
    "#     print(topic)\n",
    "    print(preprocess_string(topic[1], filters))\n",
    "    topic_dictionary[(topic[0])] = preprocess_string(topic[1], filters)\n",
    "    print(\"=====================================================================================\")  \n",
    "\n",
    "# print(topic_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40acae3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document ID 0 | Hollywood Actor | Tom Hanks | [(0, 0.06223703), (2, 0.93770015)]\n",
      "document ID 1 | Airplane Crash | Tenerife airport disaster | [(0, 0.01171259), (3, 0.024446933), (4, 0.9638311)]\n",
      "document ID 2 | Author | Frank Herbert | [(0, 0.026998373), (2, 0.5124269), (3, 0.46056244)]\n",
      "document ID 3 | Mountain | Annapurna I | [(1, 0.9999491)]\n",
      "document ID 4 | Website | PayPal | [(0, 0.98699546), (2, 0.012995434)]\n",
      "document ID 5 | US President | Thomas Jefferson | [(0, 0.9995755)]\n",
      "document ID 6 | US President | George Washington | [(0, 0.50601345), (4, 0.48996577)]\n",
      "document ID 7 | Sport | Badminton | [(1, 0.20342149), (4, 0.7896411)]\n",
      "document ID 8 | Bollywood Actor | Amitabh Bachchan | [(2, 0.45343107), (4, 0.5465597)]\n",
      "document ID 9 | Author | Ursula K. Le Guin | [(2, 0.22353771), (3, 0.7764551)]\n"
     ]
    }
   ],
   "source": [
    "# for each document, show the probabilities of topics which beyond the minimum_probability [(topic ID, probability)]\n",
    "\n",
    "for n,item in enumerate(corpus_[:10]):\n",
    "    print(\"document ID\", str(n), \"|\" ,end=\" \")\n",
    "    print(actual_clusters[n],end=\" | \")\n",
    "    print(titles[n],end=\" \")\n",
    "    print(\"|\", ldamodel.get_document_topics(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d04c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random artcile number = 9\n",
      "(2, 0.22326663) | ['game', 'film', 'award', 'time', 'year', 'state', 'player', 'includ', 'world', 'releas']\n",
      "(3, 0.7767262) | ['game', 'film', 'player', 'year', 'includ', 'plai', 'time', 'state', 'ball', 'releas']\n",
      "Author | Ursula K. Le Guin\n",
      "Ursula Kroeber Le Guin (; October 21, 1929 – January 22, 2018) was an American author best known for her works of speculative fiction, including science fiction works set in her Hainish universe, and the Earthsea fantasy series. She was first published in 1959, and her literary career spanned nearly sixty years, producing more than twenty novels and over a hundred short stories, in addition to poetry, literary criticism, translations, and children's books. Frequently described as an author of sc\n"
     ]
    }
   ],
   "source": [
    "# nth document's topic distribution\n",
    "topic_distribution = ldamodel.get_document_topics(corpus_[random_artcile])\n",
    "\n",
    "print(\"random artcile number =\", random_artcile)\n",
    "\n",
    "for topic in topic_distribution:\n",
    "    print(topic, \"|\", topic_dictionary[topic[0]])\n",
    "\n",
    "# nth document's category\n",
    "print(actual_clusters[random_artcile], \"|\", titles[random_artcile])\n",
    "\n",
    "# show the original document\n",
    "print(wikipedia_articles[random_artcile][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c6dc6",
   "metadata": {},
   "source": [
    "### Thoughts/Impressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb7c0d",
   "metadata": {},
   "source": [
    "I was surprised to find that terms such as mountain, animal, president, sport, or airplane were not part of\n",
    "the keywords of any topic in any number of times I ran the code. Changing how I preprocessed documents\n",
    "also did not impact the keywords much. The terms film and game appear in multiple topics, however.\n",
    "Changing the number of topics changed how the documents were distributed in the topics. For example, the\n",
    "article about Goose was initially associated with keywords like ‘film’ and ‘year’ when there were 20 topics\n",
    "but then was associated with ‘game’ and ‘player’ when there were 10 topics. Also, to note is that none of\n",
    "these terms should be topics that describe Goose an animal.\n",
    "While this technique is impressive, it was unable to satisfactorily extract topics from many of the Wikipedia\n",
    "articles used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
